from optimizer.optimizer import Optimizer

class SGD(Optimizer):
  def __init__(self, lr: float = 0.01, final_lr: float = 0, decay_type: str = None) -> None:
    super().__init__(lr, final_lr, decay_type)

  def _update_rule(self, **kwargs) -> None:
    update = self.lr * kwargs["grad"]
    kwargs["param"] -= update